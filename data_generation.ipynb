{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9fo0l2Al5I4aFyZXpKiV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sierrarowley/research_fall2020/blob/master/data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRt3ISXj_dtG"
      },
      "source": [
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "xbound = 10\n",
        "ybound = 10\n",
        "point_bound = 1000\n",
        "\n",
        "def generatedata():\n",
        "  db = np.empty((xbound, ybound), dtype=object)\n",
        "  # create random number of points\n",
        "  for n in range(0, random.randint(1, point_bound)):\n",
        "    x = random.randint(0, xbound-1)\n",
        "    y = random.randint(0, ybound-1)\n",
        "\n",
        "    # encrypt value of point\n",
        "    randstring = ''.join(random.choices(string.ascii_letters + string.digits, k=100))\n",
        "    enc = hash(randstring + str(x) + str(y))\n",
        "\n",
        "    # store encrypted point in the database\n",
        "    if db[x, y] == None:\n",
        "      db[x, y] = [enc]\n",
        "    # at most 10 values per point\n",
        "    elif len(db[x, y]) < 10:\n",
        "      db[x, y].append(enc)\n",
        "  \n",
        "  return db"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zK2BckrUu_J"
      },
      "source": [
        "The database is classified as either sparse (0) or dense (1). produce_leakage() performs every possible query of the database and saves the results from those queries in a list. The list is returned at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPAJ-vFAlXmm"
      },
      "source": [
        "def produce_leakage(db):\n",
        "  leakage = []\n",
        "  dense_flag = 1\n",
        "  querylen = 0\n",
        "  # all possible size rectangle queries\n",
        "  for l in range(1, xbound+1):\n",
        "    for w in range(1, ybound+1):\n",
        "      # all possible starting points of the rectangle query\n",
        "      for x in range(0, xbound):\n",
        "        # dont perform query if it is out of bounds\n",
        "        if x+l > xbound:\n",
        "          break\n",
        "        for y in range(0, ybound):\n",
        "          if y+w > ybound:\n",
        "            break\n",
        "\n",
        "          # if any point in database is None, then it is not dense\n",
        "          if db[x, y] == None:\n",
        "            dense_flag = 0\n",
        "          \n",
        "          curr_query = []\n",
        "          # loop through current query in the database\n",
        "          for i in range(x, x+l):\n",
        "            for j in range(y, y+w):\n",
        "              if db[i, j] != None:\n",
        "                curr_query.extend(db[i, j])\n",
        "\n",
        "          # add query leakage to entire list\n",
        "          if len(curr_query) != 0:\n",
        "            leakage.append(curr_query)\n",
        "          # keep track of longest query length for creating input array\n",
        "          if len(curr_query) > querylen:\n",
        "            querylen = len(curr_query)\n",
        "  \n",
        "  # shuffle to make location of queries less obvious\n",
        "  random.shuffle(leakage)\n",
        "  return (leakage, dense_flag, querylen)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfmeJEXnm13e"
      },
      "source": [
        "create_database() calls generatedata() and produce_leakage(), which creates the database and the search query leakages from that database. It then takes the returned list of leakages and transforms it into a 2D numpy array with dimensions (number of queries, length of longest query). All queries with a length shorter than the longest are padded with 0's at the end. Each index of the array represents hashed value of a point in a query. create_database() returns the numpy array, which represents one input to the neural net, and the classification of the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey_sbInNVySe"
      },
      "source": [
        "def create_database():\n",
        "  leakage, c, querylen = produce_leakage(generatedata())\n",
        "  numqueries = len(leakage)\n",
        "  \n",
        "  input = np.empty((numqueries, querylen))\n",
        "  for x in range(0, numqueries):\n",
        "    q = np.array(leakage[x]).reshape(-1,)\n",
        "    input[x] = np.pad(q, (0, querylen-len(q)), 'constant', constant_values=(0, 0))\n",
        "  \n",
        "  return (input, c)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN5fqx1naFEH"
      },
      "source": [
        "create_nn_data() creates a specified number of databases that will be used in the training and testing sets for the neural net. It pads each input array (from create_database) so they are all of the same size. It returns an array of inputs of size (number of inputs, number of queries, number of items in query) and an array of the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHF3Id949cs9"
      },
      "source": [
        "def create_nn_data(num_inputs):\n",
        "  data = []\n",
        "  labels = np.zeros((num_inputs))\n",
        "  numqueries = 0\n",
        "  querylen = 0\n",
        "  # create num_inputs databases for input\n",
        "  for n in range(0, num_inputs):\n",
        "    database, label = create_database()\n",
        "    data.append(database)\n",
        "    labels[n] = label\n",
        "\n",
        "    # find largest shape to later pad all databases to be same size\n",
        "    if database.shape[0] > numqueries:\n",
        "      numqueries = database.shape[0]\n",
        "    if database.shape[1] > querylen:\n",
        "      querylen = database.shape[1]\n",
        "  \n",
        "  inputs = np.empty((num_inputs, numqueries, querylen))\n",
        "\n",
        "  # pad each database\n",
        "  for i in range(0, len(data)):\n",
        "    d = data[i]\n",
        "    d = np.pad(d, ((0, max(numqueries - d.shape[0], 0)), (0, max(querylen - d.shape[1], 0))), 'constant', constant_values=(0, 0))\n",
        "    inputs[i] = d\n",
        "\n",
        "  return inputs, labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43K5LAOp3n4a"
      },
      "source": [
        "Time to make the neural net! I will be using tensorflow with keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QALvBCx_9oX8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BavwLyY08ZRp",
        "outputId": "4dc877a0-baee-4d09-bd22-da6d260ab497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "n = 100\n",
        "inputs, labels = create_nn_data(n)\n",
        "\n",
        "train_inputs = inputs[0:int(n*.8), :, :, None]\n",
        "train_labels = labels[0:int(n*.8)]\n",
        "test_inputs = inputs[int(n*.8):n, :, :, None]\n",
        "test_labels = labels[int(n*.8):n]\n",
        "\n",
        "print(\"train inputs: \", train_inputs.shape, \"train labels: \", train_labels.shape)\n",
        "print(\"test inputs: \", test_inputs.shape, \"test labels: \", test_labels.shape)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # keras.layers.Conv2D(32, 5, strides=(2, 2), padding='same', activation='relu', kernel_regularizer='L2'),\n",
        "    # tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\"),\n",
        "    # keras.layers.Conv2D(64, 5, strides=(2, 2), padding='same', activation='relu', kernel_regularizer='L2'),\n",
        "    keras.layers.Flatten(input_shape=(train_inputs.shape[1], train_inputs.shape[2])),\n",
        "    keras.layers.Dense(40, activation='relu'),\n",
        "    keras.layers.Dense(20, activation='relu'),\n",
        "    keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_inputs, train_labels, epochs=3)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_inputs,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train inputs:  (80, 3025, 877, 1) train labels:  (80,)\n",
            "test inputs:  (20, 3025, 877, 1) test labels:  (20,)\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 453ms/step - loss: 1359039802640433152.0000 - accuracy: 0.5250\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 453ms/step - loss: nan - accuracy: 0.6375\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 1s 406ms/step - loss: nan - accuracy: 0.5000\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_test_function.<locals>.test_function at 0x7f721ecdc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 - 0s - loss: nan - accuracy: 0.6000\n",
            "\n",
            "Test accuracy: 0.6000000238418579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k6SFEfB2KgM",
        "outputId": "573a4f05-4caa-4fd2-fd53-6eae923cb1c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "    # keras.layers.Conv2D(32, 5, strides=(2, 2), padding='same', activation='relu', kernel_regularizer='L2'),\n",
        "    # tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\"),\n",
        "    # keras.layers.Conv2D(64, 5, strides=(2, 2), padding='same', activation='relu', kernel_regularizer='L2'),\n",
        "    keras.layers.Flatten(input_shape=(train_inputs.shape[1], train_inputs.shape[2])),\n",
        "    keras.layers.Dense(40, activation='relu'),\n",
        "    keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_inputs, train_labels, epochs=3)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_inputs,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 427ms/step - loss: 1944169242056196096.0000 - accuracy: 0.5625\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 1s 425ms/step - loss: 1472428176205414400.0000 - accuracy: 0.8375\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 1s 373ms/step - loss: 645667999885295616.0000 - accuracy: 0.9000\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7f721c5d6268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 - 0s - loss: 7945690047355813888.0000 - accuracy: 0.6500\n",
            "\n",
            "Test accuracy: 0.6499999761581421\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}