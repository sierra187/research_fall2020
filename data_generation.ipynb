{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhaxek1UUFoMWKxXrL6JX/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sierrarowley/research_fall2020/blob/master/data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRt3ISXj_dtG"
      },
      "source": [
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "xbound = 10\n",
        "ybound = 10\n",
        "point_bound = 1000\n",
        "\n",
        "def generatedata():\n",
        "  db = np.empty((xbound, ybound), dtype=object)\n",
        "  # create random number of points\n",
        "  for n in range(0, random.randint(1, point_bound)):\n",
        "    x = random.randint(0, xbound-1)\n",
        "    y = random.randint(0, ybound-1)\n",
        "\n",
        "    # encrypt value of point\n",
        "    randstring = ''.join(random.choices(string.ascii_letters + string.digits, k=100))\n",
        "    enc = hash(randstring + str(x) + str(y))\n",
        "\n",
        "    # store encrypted point in the database\n",
        "    if db[x, y] == None:\n",
        "      db[x, y] = [enc]\n",
        "    # at most 10 values per point\n",
        "    elif len(db[x, y]) < 10:\n",
        "      db[x, y].append(enc)\n",
        "  \n",
        "  return db"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zK2BckrUu_J"
      },
      "source": [
        "The database is classified as either sparse (0) or dense (1). produce_leakage() performs every possible query of the database and saves the results from those queries in a list. The list is returned at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPAJ-vFAlXmm"
      },
      "source": [
        "def produce_leakage(db):\n",
        "  leakage = []\n",
        "  empty_query_rows = [] # keep track of empty rows(queries) for padding purposes\n",
        "  query_num = 0 # keep track of row(query) number for empty_query_rows\n",
        "  dense_flag = 1\n",
        "  querylen = 0\n",
        "  # all possible size rectangle queries\n",
        "  for l in range(1, xbound+1):\n",
        "    for w in range(1, ybound+1):\n",
        "      # all possible starting points of the rectangle query\n",
        "      for x in range(0, xbound):\n",
        "        # dont perform query if it is out of bounds\n",
        "        if x+l > xbound:\n",
        "          break\n",
        "        for y in range(0, ybound):\n",
        "          if y+w > ybound:\n",
        "            break\n",
        "\n",
        "          # if any point in database is None, then it is not dense\n",
        "          if db[x, y] == None:\n",
        "            dense_flag = 0\n",
        "\n",
        "          curr_query = []\n",
        "          # loop through current query in the database\n",
        "          for i in range(x, x+l):\n",
        "            for j in range(y, y+w):\n",
        "              # if a point(s) exists at this location in database, add it to the current query\n",
        "              if db[i, j] != None:\n",
        "                curr_query.extend(db[i, j])\n",
        "\n",
        "          # if query is empty, then add [-1] to represent empty         \n",
        "          if not curr_query:\n",
        "            leakage.append([-1])\n",
        "            # keep track of empty queries for padding later\n",
        "            empty_query_rows.append(query_num)\n",
        "          # else add query leakage to entire leakage list\n",
        "          else :\n",
        "            leakage.append(curr_query)\n",
        "\n",
        "          # keep track of longest query length for creating input array\n",
        "          if len(curr_query) > querylen:\n",
        "            querylen = len(curr_query)\n",
        "          query_num += 1\n",
        "\n",
        "  return (leakage, dense_flag, empty_query_rows, querylen)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfmeJEXnm13e"
      },
      "source": [
        "create_database() calls generatedata() and produce_leakage(), which creates the database and the search query leakages from that database. It then takes the returned list of leakages and transforms it into a 2D numpy array with dimensions (number of queries, length of longest query). All queries with a length shorter than the longest are padded with 0's at the end. Each index of the array represents hashed value of a point in a query. create_database() returns the numpy array, which represents one input to the neural net, and the classification of the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey_sbInNVySe"
      },
      "source": [
        "def create_database():\n",
        "  leakage, c, empty_rows, querylen = produce_leakage(generatedata())\n",
        "  numqueries = len(leakage)\n",
        "  \n",
        "  input = np.empty((numqueries, querylen))\n",
        "  for x in range(0, numqueries):\n",
        "    q = np.array(leakage[x])\n",
        "    # empty rows are padded with -1 to distinguish them\n",
        "    if x in empty_rows:\n",
        "      input[x] = np.pad(q, (0, querylen-len(q)), 'constant', constant_values=(0, -1))\n",
        "    # all other queries are padded with 0 to be same shape\n",
        "    else:\n",
        "      input[x] = np.pad(q, (0, querylen-len(q)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "  # shuffle leakage to make order and location of queries less obvious\n",
        "  np.random.shuffle(input)\n",
        "  \n",
        "  return (input, c)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN5fqx1naFEH"
      },
      "source": [
        "create_nn_data() creates a specified number of databases that will be used in the training and testing sets for the neural net. It pads each input array (from create_database) so they are all of the same size. It returns an array of inputs of size (number of inputs, number of queries, number of items in query) and an array of the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHF3Id949cs9"
      },
      "source": [
        "def create_nn_data(num_inputs):\n",
        "  data = []\n",
        "  labels = np.zeros((num_inputs))\n",
        "  numqueries = 0\n",
        "  querylen = 0\n",
        "  # create num_inputs databases for input\n",
        "  for n in range(0, num_inputs):\n",
        "    database, label = create_database()\n",
        "    data.append(database)\n",
        "    labels[n] = label\n",
        "\n",
        "    # find largest shape to later pad all databases to be same size\n",
        "    if database.shape[0] > numqueries:\n",
        "      numqueries = database.shape[0]\n",
        "    if database.shape[1] > querylen:\n",
        "      querylen = database.shape[1]\n",
        "  \n",
        "  inputs = np.empty((num_inputs, numqueries, querylen))\n",
        "\n",
        "  # pad each database\n",
        "  for i in range(0, len(data)):\n",
        "    d = data[i]\n",
        "    d = np.pad(d, ((0, 0), (0, max(querylen - d.shape[1], 0))), 'constant', constant_values=(0, 0))\n",
        "    inputs[i] = d\n",
        "\n",
        "  return inputs, labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43K5LAOp3n4a"
      },
      "source": [
        "Time to make the neural net! I will be using tensorflow with keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QALvBCx_9oX8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BavwLyY08ZRp",
        "outputId": "e6352ef6-731d-4f33-e4f5-4a0833b56389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "n = 100\n",
        "inputs, labels = create_nn_data(n)\n",
        "\n",
        "train_inputs = inputs[0:int(n*.8), :, :, None]\n",
        "train_labels = labels[0:int(n*.8)]\n",
        "test_inputs = inputs[int(n*.8):n, :, :, None]\n",
        "test_labels = labels[int(n*.8):n]\n",
        "\n",
        "print(\"train inputs: \", train_inputs.shape, \"train labels: \", train_labels.shape)\n",
        "print(\"test inputs: \", test_inputs.shape, \"test labels: \", test_labels.shape)\n",
        "\n",
        "train_inputs = np.reshape(train_inputs, (train_inputs.shape[0], -1))\n",
        "test_inputs = np.reshape(test_inputs, (test_inputs.shape[0], -1))\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # keras.layers.Conv2D(32, 5, strides=(2, 2), padding='same', activation='relu', kernel_regularizer='L2'),\n",
        "    # tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\"),\n",
        "    # keras.layers.Conv2D(64, 5, strides=(2, 2), padding='same', activation='relu', kernel_regularizer='L2'),\n",
        "    #keras.layers.Flatten(input_shape=(train_inputs.shape[1], train_inputs.shape[2])),\n",
        "    keras.layers.Dense(80, activation='relu'),\n",
        "    keras.layers.Dense(40, activation='relu'),\n",
        "    keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_inputs, train_labels, epochs=3)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_inputs,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train inputs:  (80, 3025, 869, 1) train labels:  (80,)\n",
            "test inputs:  (20, 3025, 869, 1) test labels:  (20,)\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 2s 729ms/step - loss: 1712046531953033216.0000 - accuracy: 0.4875\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 2s 722ms/step - loss: 1420283150063370240.0000 - accuracy: 0.8500\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 2s 679ms/step - loss: 654736840510668800.0000 - accuracy: 0.9375\n",
            "1/1 - 0s - loss: 4228436397159088128.0000 - accuracy: 0.6500\n",
            "\n",
            "Test accuracy: 0.6499999761581421\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}